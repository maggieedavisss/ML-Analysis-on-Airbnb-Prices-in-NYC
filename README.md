# ML Analysis on Airbnb Prices in NYC
![image](https://github.com/maggieedavisss/ML-Analysis-on-Airbnb-Prices-in-NYC/assets/151679687/14248e89-cd03-4d65-9738-4da538f9b558)


**Authors: Maggie Davis, Harper Harrell, Chris Bruce, and Sweta Balaji** 

# Reproduce 
Before you read about our machine-learning analysis, here are some directions on how to use our GitHub Repository to reproduce and further our analysis! To access the Airbnb data, you can download the data on Kaggle at `https://www.kaggle.com/datasets/dgomonov/new-york-city-airbnb-open-data` OR you can simply download the `Airbnb NYC Dataset.csv` CSV file in our GitHub Repository. Next, you can import the data and perform data cleaning by accessing the `Import Data and Data Cleaning .ipynb` notebook. Lastly, we ran 8 different machine-learning models when performing our analysis on Airbnb Pricing in NYC. The code for each of the models can be accessed in the `Machine Learning Models` folder. The models found in the folder include `Decision Tree Regression.ipynb`, `Forward Selection .ipynb`, `KNN.ipynb`, `Lasso Regression.ipynb`, `PCR.ipynb`, `PLS.ipynb`, `Random Forest.ipynb`, and `Ridge Regression.ipynb`. 

## Abstract 
The Airbnb marketplace in New York City has become one of the most popular marketplaces in the world. In order to better understand this dynamic and prestigious playground, we looked at an Airbnb dataset that included information such as id, name, host_id, host_name, neighbourhood_group, neighbourhood, latitude, longitude, room_type, price, minimum_nights, number_of_reviews, last_review, reviews_per_month, calculated_host_listings_count, and availability_365. Using 8 different machine-learning models, we wanted to see how many and to what extent each variable impacted the **price** of one night in the Big Apple. Using various methodologies, we found that a **random forest regression model** was the most predictive. Using this model, we saw that the size/privacy of the listing was most predictive, followed by the exact location of the listing (i.e. longitude and latitude). Other important variables in predicting the price of an Airbnb included the host's ID (host_id), the number of reviews the Airbnb received each month (reviews_per_month), and how many days the Airbnb was available throughout the year (availability_365). Using our model, you can enter some information about your home address to estimate the market price.

## Introduction
Travel into the United States has been on the rise for the past several decades. Following the setback in travel during the pandemic, the number of trips and amount of money spent on domestic and international travel in the country is now projected to surpass pre-pandemic levels in the coming years. Domestic business and leisure trips are projected to increase to over 2.5 billion, while international travel into the US is expected to garner an estimated $185 billion within the year 2026 alone (US Travel). As the amount of domestic and international travel into the United States increases, so has the demand for improved access to accommodation for travelers. 

Airbnb is a service that allows anyone looking to temporarily rent out their property to connect with travelers seeking accommodations. Owners are given the opportunity to earn money passively, while simultaneously offering lodging options for tourists. We believe that the growing travel industry has heightened the need for diverse and improved options for accommodation, which serves as the motivation for this study. We seek to create a model that predicts the market price of property for those interested in renting out their homes via Airbnb. To achieve this, we examine data on Airbnb listings in New York City to investigate which characteristics of Airbnb properties are most predictive of their market price and provide an approximation for how well our model can estimate the potential cost of a prospective listing. New York City’s Airbnb market serves as an interesting point of study because it possesses one of the most diverse housing markets, containing anything from the country’s most expensive luxury homes to private studio apartments. 

Our quest to identify the most predictive variables of Airbnb price using data from New York City allows us to take on the bigger task of helping owners determine which characteristics of their property are most important to consider when pricing their Airbnb rentals. We hope to provide owners with the opportunity to input data on the characteristics of their properties and give them an estimate of their rental price. We seek to satisfy owners with an appropriate price estimate for their property that accounts for its amenities and characteristics while simultaneously giving renters cost-effective options for their accommodation. 

We aim to use various feature selection models to help us determine the most important predictors of Airbnb prices. These models include Ridge and Lasso Regressions, Forward Selection, Decision Tree Regression, and Random Forest Regression. We also used models that are effective at reducing the dimensionality of our complex data such as Partial Least Squares Regression and Principal Component Regression and models generally useful for making predictions, like the KNN Regression. We predict that the feature selection models will be useful in handling the complexity and potential noisiness of our dataset and reduce the number of variables that are least predictive of Airbnb prices. More specifically, we anticipate that the Random Forest model will be useful in deciphering the most predictive variables, since it can handle the mix of numeric and categorical data present in our dataset, potential missing values and noise, and is not as prone to overfitting or error due to bias, since it aggregates the average outputs of several decisions trees. Additionally, the Random Forest model does not assume linearity between variables, whereas other algorithms, such as Lasso Regression, Ridge Regression, and Forward Selection, choose the most important predictors by modifying or assuming a linear relationship between variables. Furthermore, though Random Forest does not explicitly conduct dimensionality reduction, it is capable of selecting a subset of relevant predictors from our dataset, making it suitable to use for datasets that are high in dimensionality. Though the Partial Least Squares and Principal Component Regression algorithms are useful in dimensionality reduction, they are often more suitable in cases where there is primarily a linear relationship between the predictor and target variables, while the Random Forest can address highly complex and nonlinear relationships between variables. Moreover, KNN regression, which is useful in conducting predictions, struggles to perform efficiently in cases where there is a large number of features to parse through, whereas the Random Forest model can decipher the most important variables even in datasets of high dimension.

Though the Random Forest method appears to be one of the best approaches to building our model, there are limitations in executing the algorithm. Firstly, in order to increase the accuracy of our model, we must increase the number of decision trees. However, increasing the number of decision trees within the model may require more computing power and decrease the computing speed. Therefore, for the scope of our project and resources, we may need to be selective about the number of trees included in our model in order to prioritize computing power and speed. Another limitation is that Random Forest combines decision trees independently of each other. Therefore, though building decision trees in a particular order or combination may result in a more accurate prediction model, Random Forest does not account for the order of decision trees combined. Upon further investigation following our project, we found that research studies that have previously solved our research question have identified models more accurate than the Random Forest. However, for the purposes of our project, we have included the models within the scope of our class. Further details about the limitations of our modeling algorithms and more accurate models from previous research are outlined in the Discussions.


## **Setup: Set up the stage for your experimental results.**
Describe the dataset, including its basic statistics.
Describe the experimental setup, including what models you are going to run, what parameters you plan to use, and what computing environment you will execute on.
Describe the problem setup (e.g., for neural networks, describe the network structure that you are going to use in the experiments).

## **Results: Describe the results from your experiments.**
Main results: Describe the main experimental results you have; this is where you highlight the most interesting findings.
Supplementary results: Describe the parameter choices you have made while running the experiments. This part goes into justifying those choices.

## **Discussion: Discuss the results obtained above.** 
If your results are very good, see if you could compare them with some existing approaches that you could find online. If your results are not as good as you had hoped for, make a good-faith diagnosis about what the problem is.

## **Conclusion: In several sentences, summarize what you have done in this project.**

## **References: Put any links, papers, blog posts, or GitHub repositories that you have borrowed from/found useful here.**
